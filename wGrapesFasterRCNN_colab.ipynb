{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusZIhfVCXPY",
        "outputId": "ca5a2772-2c57-4595-ecdd-2cecab5f83ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n",
            "/content\n",
            "--2023-06-22 18:29:06--  https://zenodo.org/record/6757555/files/wGrapeUNIPD-DL%20dataset.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2391373374 (2.2G) [application/octet-stream]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip              6%[>                   ] 137.07M  24.2MB/s    eta 1m 42s "
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd content\n",
        "!wget -O data.zip https://zenodo.org/record/6757555/files/wGrapeUNIPD-DL%20dataset.zip?download=1\n",
        "!unzip data.zip\n",
        "!rm wGrapeUNIPD-DL\\ dataset/Calibrated_Images/without_Conting/Chardonnay_BBCH75_20_06_20/_DSC8819.jpg\n",
        "!rm wGrapeUNIPD-DL\\ dataset/Calibrated_Images/without_Conting/Chardonnay_BBCH75_20_06_20/_DSC8819.txt\n",
        "!rm wGrapeUNIPD-DL\\ dataset/Calibrated_Images/with_Counting/Multiple_Cultivar_BBCH83_13_08_20/_counting.txt\n",
        "!wget -O data2.zip https://zenodo.org/record/3361736/files/thsant/wgisd-1.0.0.zip?download=1\n",
        "!unzip data2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA3aMwRACgHL"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "!rm -r /vision\n",
        "\n",
        "!rm -r /datasets/wgisd/\n",
        "!rm -r /datasets/grapes/\n",
        "!mkdir /datasets\n",
        "\n",
        "!mkdir /datasets/wgisd/\n",
        "!mkdir /datasets/grapes/\n",
        "!mkdir /datasets/wgisd/images/\n",
        "!mkdir /datasets/grapes/images/\n",
        "!mkdir /datasets/wgisd/labels/\n",
        "!mkdir /datasets/grapes/labels/\n",
        "!mkdir /datasets/wgisd/labels/train\n",
        "!mkdir /datasets/grapes/labels/train\n",
        "!mkdir /datasets/wgisd/labels/val\n",
        "!mkdir /datasets/grapes/labels/val\n",
        "!mkdir /datasets/wgisd/images/train\n",
        "!mkdir /datasets/grapes/images/train\n",
        "!mkdir /datasets/wgisd/images/val\n",
        "!mkdir /datasets/grapes/images/val\n",
        "!cp /content/wGrapeUNIPD-DL\\ dataset/Calibrated_Images/*/*/*.txt /datasets/grapes/labels/train/\n",
        "!cp /content/wGrapeUNIPD-DL\\ dataset/Calibrated_Images/*/*/*.jpg /datasets/grapes/images/train/\n",
        "!cp /content/wGrapeUNIPD-DL\\ dataset/Calibrated_Images/*/*/*.JPG /datasets/grapes/images/train/\n",
        "\n",
        "!cp /content/thsant-wgisd-ab223e5/data/*.txt /datasets/wgisd/labels/train/\n",
        "!cp /content/thsant-wgisd-ab223e5/data/*.jpg /datasets/wgisd/images/train/\n",
        "!cp /content/thsant-wgisd-ab223e5/data/*.JPG /datasets/wgisd/images/train/\n",
        "\n",
        "!rm /datasets/grapes/labels/train/_DSC8819.txt\n",
        "!rm /datasets/grapes/images/train/_DSC8819.jpg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz3CBipiCanB"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "import os\n",
        "import random\n",
        "lst = sorted(os.listdir(\"/datasets/grapes/labels/train/\"))\n",
        "lst_imgs = sorted(os.listdir(\"/datasets/grapes/images/train/\"))\n",
        "\n",
        "indici= random.sample(range(0, len(lst)-1), int(len(lst)*0.2))\n",
        "print(len(lst), len(lst_imgs))\n",
        "print(indici)\n",
        "for i in indici:\n",
        "  str_lab= \"/datasets/grapes/labels/train/\"+lst[i]\n",
        "  str_img= \"/datasets/grapes/images/train/\"+lst_imgs[i]\n",
        "  !mv $str_lab /datasets/grapes/labels/val/\n",
        "  !mv $str_img /datasets/grapes/images/val/\n",
        "\n",
        "lst = sorted(os.listdir(\"/datasets/wgisd/labels/train/\"))\n",
        "lst_imgs = sorted(os.listdir(\"/datasets/wgisd/images/train/\"))\n",
        "\n",
        "indici= random.sample(range(0, len(lst)-1), int(len(lst)*0.2))\n",
        "print(len(lst), len(lst_imgs))\n",
        "print(indici)\n",
        "for i in indici:\n",
        "  str_lab= \"/datasets/wgisd/labels/train/\"+lst[i]\n",
        "  str_img= \"/datasets/wgisd/images/train/\"+lst_imgs[i]\n",
        "  !mv $str_lab /datasets/wgisd/labels/val/\n",
        "  !mv $str_img /datasets/wgisd/images/val/\n",
        "\n",
        "print(len(os.listdir(\"/datasets/grapes/labels/train\")), len(os.listdir(\"/datasets/grapes/labels/val\")))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvGjffwMIsNX"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.15.1\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo_h1cRCbhl7"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRb3zTE6Dm4v"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U albumentations\n",
        "!pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNAjb6Z8HQtz"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from albumentations.core.composition import Compose\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torch.nn as nn\n",
        "import albumentations as A\n",
        "\n",
        "from albumentations.augmentations.geometric.resize import RandomScale\n",
        "import albumentations as A\n",
        "import random\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 5\n",
        "IMAGE_WIDTH = 1216\n",
        "IMAGE_HEIGHT = 1216\n",
        "FILES_DIR_WGISD = \"/datasets/wgisd\"\n",
        "FILES_DIR_WGRAPE = \"/datasets/grapes\"\n",
        "print(DEVICE)\n",
        "\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(p = 0.5),\n",
        "        A.Blur(blur_limit=3, p=0.1),\n",
        "        A.RandomScale(0.5, p=1.0),\n",
        "        A.ColorJitter(0.0, 0.2, 0.2, p=0.2),\n",
        "        ToTensorV2(p = 1.0)\n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return Compose(\n",
        "      [\n",
        "        ToTensorV2(p=1.0)\n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_7Zv9SUG9ah"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GrapeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, width, height, transforms = None):\n",
        "        self.transforms = transforms\n",
        "        self.root = root\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        self.imgs=[]\n",
        "        boxes_files=[]\n",
        "        for root, dirs, files in os.walk(self.root):\n",
        "            for file in files:\n",
        "                if file.endswith(\".txt\") and not file.startswith(\"_counting\"):\n",
        "                    boxes_files.append(os.path.join(root, file))\n",
        "                if file.endswith(\".jpg\") or file.endswith(\".JPG\")  :\n",
        "                    self.imgs.append(os.path.join(root, file))\n",
        "                if file.startswith(\"_counting\"):\n",
        "                   self.conting_file = os.path.join(root, \"_counting.txt\")\n",
        "        self.imgs = sorted(self.imgs)\n",
        "        boxes_files = sorted(boxes_files)\n",
        "        self.boxes = boxes_files\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image and boxes\n",
        "        img_path = self.imgs[idx]\n",
        "        box_path = self.boxes[idx]\n",
        "        # image elaboration\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img = cv2.resize(img, (self.width, self.height), cv2.INTER_AREA)\n",
        "        img /= 255.0\n",
        "\n",
        "        height, width, _ = img.shape\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        labels=[]\n",
        "        boxes=[]\n",
        "        with open(box_path) as f:\n",
        "            for line in f:\n",
        "                labels.append(1)\n",
        "\n",
        "                elems=[float(x) for x in line.split(' ')]\n",
        "\n",
        "                x_center = int(elems[1] * width)\n",
        "                y_center = int(elems[2] * height)\n",
        "                box_wt = int(elems[3] * width)\n",
        "                box_ht = int(elems[4] * height)\n",
        "\n",
        "                x_min = max(0.0, x_center - box_wt/2)\n",
        "                x_max  = min(width, x_center + box_wt/2)\n",
        "                y_min = max(0.0, y_center - box_ht/2)\n",
        "                y_max  = min(height, y_center + box_ht/2)\n",
        "\n",
        "                boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels= torch.as_tensor(labels, dtype=torch.int64)\n",
        "        iscrowd= torch.zeros((boxes.shape[0],),dtype=torch.int64)\n",
        "        area = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])\n",
        "        target={}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"area\"] = area\n",
        "        target[\"labels\"]= labels\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = img,\n",
        "                                bboxes = target['boxes'],\n",
        "                                labels = labels)\n",
        "            img = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K70F_gcHXiv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_img_bbox(img, target, prediction):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, ax = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  img=img.permute(1,2,0)\n",
        "  ax.imshow(img)\n",
        "\n",
        "  for box in (target['boxes']):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'black',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  for box in (prediction['boxes']):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 1,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    ax.add_patch(rect)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_img_bbox_alone(img, target):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, ax = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  img=img.permute(1,2,0)\n",
        "  ax.imshow(img)\n",
        "\n",
        "  for box in (target['boxes']):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'black',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "files_path=FILES_DIR_WGISD\n",
        "dataset_train = GrapeDataset(files_path, IMAGE_WIDTH, IMAGE_HEIGHT, get_transform(train=True))\n",
        "dataset_test = GrapeDataset(files_path, IMAGE_WIDTH, IMAGE_HEIGHT,  get_transform(train=False))\n",
        "print(\"Number of Dataset Images: \", len(dataset_test))\n",
        "n = random.randint(0,len(dataset_test)-1)\n",
        "img, target = dataset_train[n]\n",
        "plot_img_bbox_alone(img, target)\n",
        "img, target = dataset_test[n]\n",
        "plot_img_bbox_alone(img, target)\n",
        "\n",
        "dataset_wgrape = GrapeDataset(FILES_DIR_WGRAPE, IMAGE_WIDTH, IMAGE_HEIGHT, transforms=get_transform(train=True))\n",
        "dataset_test_wgrape = GrapeDataset(FILES_DIR_WGRAPE, IMAGE_WIDTH, IMAGE_HEIGHT,  transforms=get_transform(train=False))\n",
        "\n",
        "dataset_wgisd = GrapeDataset(FILES_DIR_WGISD, IMAGE_WIDTH, IMAGE_HEIGHT, transforms=get_transform(train=True))\n",
        "dataset_test_wgisd = GrapeDataset(FILES_DIR_WGISD, IMAGE_WIDTH, IMAGE_HEIGHT,  transforms=get_transform(train=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHUQ761WCKXx"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import torch\n",
        "import numpy\n",
        "\n",
        "loss_objectness = []\n",
        "loss_box_reg = []\n",
        "loss_classifier= []\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, device, epoch):\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, targets) in (tepoch := tqdm(enumerate(data_loader), unit=\"batch\", total=len(data_loader))):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "            if len(targets)==0:\n",
        "                  continue\n",
        "            # Step 1: send the image to the required device.\n",
        "            # Images is a list of B images (where B = batch_size of the DataLoader).\n",
        "            images = list(img.to(device) for img in images)\n",
        "            # Step 2: send each target to the required device\n",
        "            # Targets is a dictionary of metadata. each (k,v) pair is a metadata\n",
        "            # required for training.\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            model_time = time.time()\n",
        "            loss_dict=model(images, targets)\n",
        "\n",
        "            model_time = time.time() - model_time\n",
        "            loss_objectness.append(loss_dict[\"loss_objectness\"].cpu().detach().numpy())\n",
        "            loss_box_reg.append(loss_dict[\"loss_box_reg\"].cpu().detach().numpy())\n",
        "            loss_classifier.append(loss_dict[\"loss_classifier\"].cpu().detach().numpy())\n",
        "            # Step 3. backward on loss.\n",
        "            # Normally, you would obtain the loss from the model.forward()\n",
        "            # and then just call .bacward() on it.\n",
        "            # In this case, for each task, you have a different loss, due to\n",
        "            # different error metrics adopted by the tasks.\n",
        "            # One typical approach is to combine all the losses to one single loss,\n",
        "            # and then then backward that single loss.\n",
        "            # In this way you can adjust the weight of the different tasks,\n",
        "            # multiplying each loss for a hyperparemeter.\n",
        "            # E.G.:\n",
        "            #       final_loss = loss_1 + gamma*(alpha*loss_2 + beta*loss_3)\n",
        "            # In this case, we want to sum up all the losses.\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            tepoch.set_postfix(loss=losses.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1etnzzXCCKXy"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "# use our dataset and defined transformations\n",
        "def redefine_dataset(path, size_image_width, size_image_height):\n",
        "    dataset = GrapeDataset(path, size_image_width, size_image_height, transforms=get_transform(train=True))\n",
        "    dataset_test = GrapeDataset(path, size_image_width, size_image_height,  transforms=get_transform(train=False))\n",
        "\n",
        "    # train test split\n",
        "    torch.manual_seed(random.randint(0,100))\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    test_split = 0.2\n",
        "    print(indices)\n",
        "    tsize = int(len(dataset)*test_split)\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "    print(len(dataset))\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "    print(len(dataset_test))\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=utils.collate_fn,\n",
        "    )\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=utils.collate_fn,\n",
        "    )\n",
        "\n",
        "    return data_loader, data_loader_test\n",
        "\n",
        "\n",
        "data_loader_wgisd, data_loader_test_wgisd = redefine_dataset(FILES_DIR_WGISD, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "print(str(len(data_loader_wgisd))+\" \"+str(len(data_loader_test_wgisd)))\n",
        "data_loader_wgrape, data_loader_test_wgrape = redefine_dataset(FILES_DIR_WGRAPE, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "print(str(len(data_loader_wgrape))+\" \"+str(len(data_loader_test_wgrape)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s5zaWr4CKXy"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNConvFCHead\n",
        "from torchvision.models.detection.faster_rcnn import RPNHead\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "def create_model(num_classes, backbone=\"resnet_50\"):\n",
        "\n",
        "    model=None\n",
        "\n",
        "    if backbone == \"vgg16\":\n",
        "        vgg_net = torchvision.models.vgg13(pretrained=True)\n",
        "        ft_backbone = vgg_net.features\n",
        "        ft_backbone.out_channels = 512\n",
        "        anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "        )\n",
        "        # Feature maps to perform RoI cropping.\n",
        "        # If backbone returns a Tensor, `featmap_names` is expected to\n",
        "        # be [0]. We can choose which feature maps to use.\n",
        "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "            featmap_names=['0'],\n",
        "            output_size=7,\n",
        "            sampling_ratio=2\n",
        "        )\n",
        "        # Final Faster RCNN model.\n",
        "        model = FasterRCNN(\n",
        "            backbone=ft_backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=anchor_generator,\n",
        "            box_roi_pool=roi_pooler\n",
        "        )\n",
        "\n",
        "    elif backbone == \"vgg19\":\n",
        "        vgg_net = torchvision.models.vgg19(pretrained=True)\n",
        "        ft_backbone = vgg_net.features\n",
        "        ft_backbone.out_channels = 512\n",
        "        anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "        )\n",
        "        # Feature maps to perform RoI cropping.\n",
        "        # If backbone returns a Tensor, `featmap_names` is expected to\n",
        "        # be [0]. We can choose which feature maps to use.\n",
        "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "            featmap_names=['0'],\n",
        "            output_size=7,\n",
        "            sampling_ratio=2\n",
        "        )\n",
        "        # Final Faster RCNN model.\n",
        "        model = FasterRCNN(\n",
        "            backbone=ft_backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=anchor_generator,\n",
        "            box_roi_pool=roi_pooler\n",
        "        )\n",
        "    elif backbone == \"resnet18_fpn\":\n",
        "        backbone = resnet_fpn_backbone('resnet18', True)\n",
        "        model = FasterRCNN(backbone, num_classes)\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
        "\n",
        "    elif backbone == \"resnet50_fpn\":\n",
        "        backbone = resnet_fpn_backbone('resnet50', True)\n",
        "        model = FasterRCNN(backbone, num_classes)\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
        "\n",
        "    elif backbone == \"resnet101_fpn\":\n",
        "        backbone = resnet_fpn_backbone('resnet101', True)\n",
        "        model = FasterRCNN(backbone, num_classes)\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
        "    else:\n",
        "        print(\"Error Wrong unsupported Backbone\")\n",
        "        return\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQKQso--IWyG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torch\n",
        "from torchvision import transforms as torchtrans\n",
        "from detection.engine import train_one_epoch, evaluate\n",
        "import detection.utils\n",
        "import detection.transforms as T\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
        "from torchvision.models.detection import RetinaNet, RetinaNet_ResNet50_FPN_V2_Weights, FasterRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models import ResNet50_Weights, resnet101\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, RetinaNetRegressionHead\n",
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "\n",
        "# train on gpu if available\n",
        "num_classes = 2 # one class (class 0) is dedicated to the \"background\"\n",
        "backbones=[\"vgg16\",\"vgg19\",\"resnet18_fpn\", \"resnet50_fpn\", \"resnet101_fpn\"]\n",
        "# backbones=[\"resnet50_fpn\"]\n",
        "coco_tests = []\n",
        "fps_tests = []\n",
        "\n",
        "\n",
        "lr=0.005\n",
        "coco = None\n",
        "metric_logger=[]\n",
        "for backbone in backbones:\n",
        "    # get the model using our helper function\n",
        "    model = create_model(num_classes, backbone=backbone)\n",
        "    # model = torch.load(\"weights/wgisd_frcnn.pth\")\n",
        "    \n",
        "    # move model to the right device\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    print(len(params))\n",
        "\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=lr*0.1)\n",
        "\n",
        "    # and a learning rate scheduler which decreases the learning rate by\n",
        "    # 10x every 3 epochs\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        gamma=0.1,\n",
        "        step_size=3\n",
        "    )\n",
        "    for epoch in range(EPOCHS):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # training for one epoch\n",
        "        train_one_epoch(model, optimizer, data_loader_wgisd, DEVICE, epoch, 10)\n",
        "        lr_scheduler.step()\n",
        "        coco = evaluate(model, data_loader_test_wgisd, device=DEVICE)\n",
        "        # evaluate on the test dataset\n",
        "\n",
        "    #evaluate fps\n",
        "    img, target = dataset_test[0]\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img.to(DEVICE)])[0]\n",
        "    # get the end time\n",
        "    end_time = time.time()\n",
        "    # get the fps\n",
        "    fps = 1 / (end_time - start_time)\n",
        "    print(fps)\n",
        "    \n",
        "    fps_tests.append(fps)\n",
        "    coco_tests.append(coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "godXEIj-CKX0"
      },
      "outputs": [],
      "source": [
        "coco = evaluate(model, data_loader_test_wgisd, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w00Xa6BOCKX0"
      },
      "outputs": [],
      "source": [
        "#F1 = (2 * precision * recall)/(precision+recall)\n",
        "for i in range(0,len(backbones)):\n",
        "    all_precision = coco_tests[i].coco_eval['bbox'].eval['precision']\n",
        "    map = coco_tests[i].coco_eval['bbox'].stats[1]\n",
        "\n",
        "    pr_5 = all_precision[0, :, 0, 0, 2] # data for IoU@0.5\n",
        "    x = np.arange(0, 1.01, 0.01)\n",
        "    plt.plot(x, pr_5, label=backbones[i]+str(\" {:.3f} mAP@0.5\".format(map)), linewidth=2)\n",
        "\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.xticks(np.arange(0, 1.0001, 0.1))\n",
        "plt.yticks(np.arange(0, 1.0001, 0.1))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#F1 = (2 * precision * recall)/(precision+recall)\n",
        "for i in range(0,len(backbones)):\n",
        "    all_precision = coco_tests[i].coco_eval['bbox'].eval['precision']\n",
        "    map = coco_tests[i].coco_eval['bbox'].stats[1]\n",
        "    plt.plot(fps_tests[i], map, label=backbones[i]+str(\" {:.3f} mAP@0.5\".format(map)), linewidth=2, marker='o')\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlim(0, max(fps_tests))\n",
        "plt.ylim(0, 1)\n",
        "plt.title('COCO mAP - FPS')\n",
        "plt.xlabel('FPS')\n",
        "plt.ylabel('COCO mAP')\n",
        "plt.xticks(np.arange(0, max(fps_tests)+0.0001, 0.1))\n",
        "plt.yticks(np.arange(0, 1.0001, 0.1))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# the function takes the original prediction and the iou threshold.\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'].cpu(), orig_prediction['scores'].cpu(), iou_thresh)\n",
        "    \n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'].cpu()[keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'].cpu()[keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'].cpu()[keep]\n",
        "    \n",
        "    return final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP91BvCCwquz"
      },
      "outputs": [],
      "source": [
        "img, target = dataset_test[1]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(DEVICE)])[0]\n",
        "# get the end time\n",
        "end_time = time.time()\n",
        "# get the fps\n",
        "fps = 1 / (end_time - start_time)\n",
        "print(fps)\n",
        "\n",
        "print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.3)\n",
        "\n",
        "plot_img_bbox(img, target, nms_prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
